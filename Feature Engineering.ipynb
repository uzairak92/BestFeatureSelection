{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MiMTc5_NNPj"
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QP1q2srGNNPo"
   },
   "outputs": [],
   "source": [
    "# If additional packages are needed but are not installed by default, uncomment the last two lines of this cell\n",
    "# and replace <package list> with a list of additional packages.\n",
    "# This will ensure the notebook has all the dependencies and works everywhere\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install <package list>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZnKy7tPyNNPs"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 101)\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DZrs3tbNNPv"
   },
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErDxdcsoNNPx"
   },
   "source": [
    "Column | Description\n",
    ":---|:---\n",
    "`id` | Unique identifier for each booking.\n",
    "`lead_time` | Time between booking date and reservation date (in days)\n",
    "`arrival_week` | Week number of the arrival date.\n",
    "`duration` | Booking duration (in Days)\n",
    "`prev_cancel` | Number of previous bookings that were cancelled by the customer prior to the current booking.\n",
    "`booking_changes` | Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation. \n",
    "`waiting_period` | Waiting period for booking confirmation (in Days)\t\n",
    "`per_Day_price` | Per night booking price (in US $).\n",
    "`parking` | Number of car parking spaces required by the customer.\n",
    "`special_request` | Number of special requests made by the customer.\n",
    "`segment` | Market segment designation. In categories, “TA” means “Travel Agents” and “TO” means “Tour Operators”.\n",
    "`deposit` | Whether the customer made a deposit to guarantee the booking.\n",
    "`cust_type` | Type of booking, assuming one of four categories.\n",
    "`is_cancelled` |Value indicating if the booking was cancelled (1) or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knONIoqcNNPy"
   },
   "source": [
    "## Data Wrangling & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRLzBBNNNNPz"
   },
   "outputs": [],
   "source": [
    "# The dataset is already loaded below\n",
    "data = pd.read_csv(\"\")  # add path to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61JhPEGiNNP4",
    "outputId": "48dac540-e9ac-49a2-cfcd-575b92b3ffc9"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KwZ-0ykHNNP-",
    "outputId": "31813dc0-53be-4848-e86a-60e5efb46e07"
   },
   "outputs": [],
   "source": [
    "#Explore columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ekzXg7HBNNQB",
    "outputId": "3d4d826d-09c2-44d7-fb30-d7932f90dff9"
   },
   "outputs": [],
   "source": [
    "#Description\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "776AnflSNNQD"
   },
   "source": [
    "## Visualization, Modeling, Machine Learning\n",
    "\n",
    "Build a classification model and to determine whether a customer will cancel a booking. Please explain the findings effectively to technical and non-technical audiences using comments and visualizations, if appropriate.\n",
    "- **Build an optimized model that effectively solves the business problem.**\n",
    "- **The model's performance will be evaluated on the basis of accuracy.**\n",
    "- **Read the test.csv file and prepare features for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwO5c1ABNNQE",
    "outputId": "6246cf4b-42f0-4805-c1c9-8b3086af2f1b"
   },
   "outputs": [],
   "source": [
    "#Loading Test data\n",
    "test_data = pd.read_csv(\"\")  # add path to file\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAmSb7AfNNQI"
   },
   "source": [
    "Key Takeaways from the get go: 1)id is unique and as such cannot be used during the classification process, so will drop that 2)output label is is_cancelled 3)segment, deposit and cust_type are categorical features and we will need to test their corelation/independence separately. We first check the distribution of our output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0I2lsH-NNQP"
   },
   "outputs": [],
   "source": [
    "#Observe output class balance\n",
    "ax = sns.countplot(y,label=\"\")\n",
    "Y, N = y.value_counts()\n",
    "print('Yes: ',Y)\n",
    "print('No: ',N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't show a severe class imbalance, so we won't have to bother with class imbalance fixes (like selective upsampling and downsampling). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IqxVwWBwNNQS"
   },
   "source": [
    "\n",
    "**Describe the the most important features of the model to management.**\n",
    "\n",
    "> #### Task:\n",
    "- **Visualize the top 10 features and their feature importance.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a basic idea of our feature selection, we will do a simple Random Forest feature classification feature importance plot, wihtout any significant transformation to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest for feature importance on a classification problem\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X = data.drop(['',''], axis=1)  # drop unneeded features\n",
    "y = data.is_cancelled\n",
    "\n",
    "#Encode categorical values\n",
    "coder1 = LabelEncoder()\n",
    "labels1 = coder1.fit_transform(X[''])\n",
    "X[''] = labels1\n",
    "coder2= LabelEncoder()\n",
    "labels2 = coder2.fit_transform(X[''])\n",
    "X[''] = labels2\n",
    "coder3= LabelEncoder()\n",
    "labels3 = coder3.fit_transform(X[''])\n",
    "X[''] = labels3\n",
    "\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    \n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears feature 5 (waiting_period) out of the 11 features is least important, although followed closely by feature 7 (parking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin our own analysis and observe if any features can be dropped. To begin our data transformation and detailed analysis, we first work with the numerical input features. We will first normalize our data and obtain a box plot for the numerical features to get a quick overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_drop = ['','','','',''] #numerical features to be dropped\n",
    "x_num = data.drop(num_drop,axis = 1 )\n",
    "y = data.is_cancelled\n",
    "x_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNMb2ZGCNNQT"
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "pltdata = x_num\n",
    "data2 = (pltdata - pltdata.mean()) / (pltdata.std())\n",
    "pltdata = pd.concat([y,data2],axis=1)\n",
    "pltdata = pd.melt(pltdata,id_vars=\"is_cancelled\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "#Box PLot\n",
    "sns.boxplot(x=\"features\", y=\"value\", hue=\"is_cancelled\", data=pltdata)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plot shows majority of values for prev_cancel, booking_changes and parking are 0, with handful of sever outliers. While this may show overall importance of those three features, instead of outright elliminating them, we will first observe their correlation using a pair grid plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UCGui6nNNQV"
   },
   "outputs": [],
   "source": [
    "#Pair grid plot\n",
    "sns.set(style=\"white\")\n",
    "df = x_num.loc[:,['prev_cancel','booking_changes','parking']]\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_diag(sns.kdeplot, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks all three of those features might be correlated. We however will not draw final conclusions yet. Although at this point it looks like parking can be dropped during our feature selection as it had ultimately low importance from our very first plot. We will proceeed to drawing a final correlation plot using heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat map\n",
    "f,ax = plt.subplots(figsize=(9, 9))\n",
    "sns.heatmap(x_num.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it appears there might not be a strong correlation link between any of the numerical input features, so we will not be dropping any of those features. We will now proceed to categorical feature selection. In order to get a clear idea of unique values for each, we will also get a short summary for the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_drop = ['','','','','','','','','','','']  # categorical features to be dropped\n",
    "x_cat = data.drop(cat_drop,axis = 1)\n",
    "\n",
    "#Summary of complete data\n",
    "\n",
    "def data_summary(df):\n",
    "\n",
    "    df = pd.DataFrame({'type': df.dtypes,\n",
    "                       'null_values': df.isna().sum(),\n",
    "                       'null_values (%)': (df.isna().sum() / df.shape[0]) * 100,\n",
    "                       'unique': df.nunique()})\n",
    "    return df\n",
    "\n",
    "print(data_summary(x_cat))\n",
    "print(x_cat.segment.unique())\n",
    "print(x_cat.deposit.unique())\n",
    "print(x_cat.cust_type.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to begin testing the cateforical features, we will first need to encode our data (similar to what we used in the initial feature plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform and map pokemon generations\n",
    "segcoder = LabelEncoder()\n",
    "seg_labels = segcoder.fit_transform(x_cat['segment'])\n",
    "x_cat['segment'] = seg_labels\n",
    "depcoder = LabelEncoder()\n",
    "dep_labels = depcoder.fit_transform(x_cat['deposit'])\n",
    "x_cat['deposit'] = dep_labels\n",
    "custcoder = LabelEncoder()\n",
    "cust_labels = custcoder.fit_transform(x_cat['cust_type'])\n",
    "x_cat['cust_type'] = cust_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the correlation, we use chi square value to determine the importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Chisquare\n",
    "from scipy.stats import chi2_contingency\n",
    "def chisq_of_df_cols(df, c1, c2):\n",
    "    groups = df.groupby([c1, c2]).size()\n",
    "    ctsum = groups.unstack(c1)\n",
    "    #print(ctsum)  # prints the stacked table generated for Chisquare\n",
    "    # Fill null values with 0s, just incase a mishape happened for error correction\n",
    "    return(chi2_contingency(ctsum.fillna(0)))\n",
    "\n",
    "\n",
    "def chisq_of_df(x_cat):\n",
    "    for column in x_cat:\n",
    "        column1 = str(column)\n",
    "        if column1 == 'is_cancelled':\n",
    "            continue\n",
    "        print(column1)  # Print the column name\n",
    "        chisquared_value = chisq_of_df_cols(x_cat, 'is_cancelled', column1)        \n",
    "        print('test statistics: ', chisquared_value[0])\n",
    "        print('P-value: ', chisquared_value[1])\n",
    "\n",
    "        \n",
    "#add the ouput label to test \n",
    "x_cat['is_cancelled'] = data.is_cancelled\n",
    "chisq_value = chisq_of_df(x_cat)  # chisuqared values of categorical data       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value for all three features are <0.05, so all categorical features are significant. Based on all the analysis so far, we will conclude that only id and parking will need to be dropped from our input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now begin to create our model, which we will use to predict with the test.csv data. We decided to use Random Forest as our classifier and used pandas get_dummies to one-hot encode our categorical features instead of label encoding. We use 1000 decision trees as the hyperparameter for our model, keeping all others as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define dataset\n",
    "X = data.drop(['id','parking','is_cancelled'], axis=1)\n",
    "y = data.is_cancelled\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We eventually landed with an accuracy of 84.55%, which we are happy with. We will now being predicting the test.csv data with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cqv9hOnfNNQZ"
   },
   "source": [
    "> #### Task:\n",
    "- **Submit the predictions on the test dataset using your optimized model** <br/>\n",
    "    For each record in the test set (`test.csv`), you must predict whether a customer will cancel his booking or not. You should submit a CSV file with a header row and one row per test entry. \n",
    "\n",
    "The file (`submissions.csv`) should have exactly 2 columns:\n",
    "   - **id**\n",
    "   - **is_cancelled**\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C2FpSVKqNNQa"
   },
   "outputs": [],
   "source": [
    "#prep and one-hot encode prediction data\n",
    "pred_data = test_data.set_index(['id'])\n",
    "pred_data = pred_data.drop(['parking'],axis=1)\n",
    "pred_data = pd.get_dummies(pred_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our test data, it appears that the deposit column's \"refundable\" category is not present in the test.csv data (therefore get_dummies doesnt create its respective column) and as a result an error kept appearing. To circumvent this, I created its respective blank column for test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.insert(17, 'deposit_Refundable', pred_data.cust_type_Group)\n",
    "pred_data['deposit_Refundable'] = pred_data['deposit_Refundable']*0\n",
    "pred_data.deposit_Refundable.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-ZgcN1CNNQd"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(pred_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the predictions are made, we will now save the output (id,is_cancelled) into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_data[['id']].copy() #copy the id column from test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['is_cancelled'] = predictions\n",
    "print(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K2uCUWhNNQj"
   },
   "outputs": [],
   "source": [
    "#Submission\n",
    "submission_df.to_csv('submissions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
